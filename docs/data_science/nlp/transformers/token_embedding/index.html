<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Token Embedding#Token embeddings provide a numerical representation for words, capturing their semantic meaning and allowing machines to process and understand text.
This article delves into the mechanics of token embeddings, from tokenization to weight adjustments in neural networks.
Contents#Tokenizing Embedding Forward Backward WrappingUp 1.Tokenizing#Assume that We have vocabulary items (or tokens)
token_size = 5 vocas = [f&#34;voca{i}&#34; for i in range(token_size)] tokens = [i for i in range(token_size)] print(&#34;\nvocas:\n&#34;, vocas) print(&#34;\ntokenized_vocas:\n&#34;, tokens) vocas:[&#39;voca0&#39;, &#39;voca1&#39;, &#39;voca2&#39;, &#39;voca3&#39;, &#39;voca4&#39;]tokenized_vocas:[0, 1, 2, 3, 4]2.">
<meta name="theme-color" content="#FFFFFF">
<meta name="color-scheme" content="light dark"><meta property="og:title" content="" />
<meta property="og:description" content="Token Embedding#Token embeddings provide a numerical representation for words, capturing their semantic meaning and allowing machines to process and understand text.
This article delves into the mechanics of token embeddings, from tokenization to weight adjustments in neural networks.
Contents#Tokenizing Embedding Forward Backward WrappingUp 1.Tokenizing#Assume that We have vocabulary items (or tokens)
token_size = 5 vocas = [f&#34;voca{i}&#34; for i in range(token_size)] tokens = [i for i in range(token_size)] print(&#34;\nvocas:\n&#34;, vocas) print(&#34;\ntokenized_vocas:\n&#34;, tokens) vocas:[&#39;voca0&#39;, &#39;voca1&#39;, &#39;voca2&#39;, &#39;voca3&#39;, &#39;voca4&#39;]tokenized_vocas:[0, 1, 2, 3, 4]2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/token_embedding/" /><meta property="article:section" content="docs" />


<title>Token Embedding | Discovering Data Science with Crimson</title>
<link rel="manifest" href="/data_science_blog/manifest.json">
<link rel="icon" href="/data_science_blog/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/data_science_blog/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css" integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz&#43;OQteg=" crossorigin="anonymous">
  <script defer src="/data_science_blog/flexsearch.min.js"></script>
  <script defer src="/data_science_blog/en.search.min.a2040b3bb4198ad5c945338be7050307a399d764565c1e3bb5d4f76954ca9508.js" integrity="sha256-ogQLO7QZitXJRTOL5wUDB6OZ12RWXB47tdT3aVTKlQg=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/data_science_blog/"><span>Discovering Data Science with Crimson</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>Data Science</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b2f022745f8088cd643ee6d2da035e6b" class="toggle" checked />
    <label for="section-b2f022745f8088cd643ee6d2da035e6b" class="flex justify-between">
      <a role="button" class="">Natural Language Process</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d06abac363844c6f23395806fd3fb26d" class="toggle" checked />
    <label for="section-d06abac363844c6f23395806fd3fb26d" class="flex justify-between">
      <a role="button" class="">Transformers</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/data_science_blog/docs/data_science/nlp/transformers/calculate_attention/" class="">Calculate Attention</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/data_science_blog/docs/data_science/nlp/transformers/position_embedding/" class="">Position Embedding</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/data_science_blog/docs/data_science/nlp/transformers/token_embedding/" class="active">Token Embedding</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>











  
<ul>
  
  <li>
    <a href="/data_science_blog/posts/"  >
        Blog
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/data_science_blog/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Token Embedding</strong>

  <label for="toc-control">
    
    <img src="/data_science_blog/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#contents">Contents</a></li>
    <li><a href="#1tokenizing">1.Tokenizing</a></li>
    <li><a href="#2embedding">2.Embedding</a></li>
    <li><a href="#3forwardpropagation">3.ForwardPropagation</a></li>
    <li><a href="#4backpropagation">4.BackPropagation</a></li>
    <li><a href="#5-wrapping-up">5. Wrapping Up</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="token-embedding">
  Token Embedding
  <a class="anchor" href="#token-embedding">#</a>
</h1>
<p>Token embeddings provide a numerical representation for words, capturing their semantic meaning and allowing machines to process and understand text.</p>
<p>This article delves into the mechanics of token embeddings, from tokenization to weight adjustments in neural networks.</p>
<h2 id="contents">
  Contents
  <a class="anchor" href="#contents">#</a>
</h2>
<ul>
<li><a href="#1tokenizing">Tokenizing</a></li>
<li><a href="#2embedding">Embedding</a></li>
<li><a href="#3forwardpropagation">Forward</a></li>
<li><a href="#4backpropagation">Backward</a></li>
<li><a href="#5-wrapping-up">WrappingUp</a></li>
</ul>
<hr>
<h2 id="1tokenizing">
  1.Tokenizing
  <a class="anchor" href="#1tokenizing">#</a>
</h2>
<p>Assume that We have vocabulary items (or tokens)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>token_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>vocas <span style="color:#f92672">=</span> [<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;voca</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(token_size)]
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> [i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(token_size)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">vocas:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, vocas)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">tokenized_vocas:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, tokens)
</span></span></code></pre></div><pre><code>vocas:
 ['voca0', 'voca1', 'voca2', 'voca3', 'voca4']

tokenized_vocas:
 [0, 1, 2, 3, 4]
</code></pre>
<hr>
<h2 id="2embedding">
  2.Embedding
  <a class="anchor" href="#2embedding">#</a>
</h2>
<p>A Embedding class is defined.
All the things will be explained.</p>
<p>Skip understanding for now.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TokenEmbedding</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, vocab_size, d_embed, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>vocab_size <span style="color:#f92672">=</span> vocab_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_embed <span style="color:#f92672">=</span> d_embed
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> learning_rate
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Initialize the weight matrix randomly</span>
</span></span><span style="display:flex;"><span>        np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embedding_weights <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, size<span style="color:#f92672">=</span>(vocab_size, d_embed))<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float64)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Save the last input for backward pass</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>last_input <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>grad_weights <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Save the input for the backward pass</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>last_input <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Use the input indices to look up the corresponding vectors in the weight matrix</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>embedding_weights[x]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, gradient_output):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Initialize a gradient matrix for the weights, filled with zeros</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>grad_weights <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros_like(self<span style="color:#f92672">.</span>embedding_weights)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Accumulate the gradients at the positions corresponding to the input indices</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i, index <span style="color:#f92672">in</span> enumerate(self<span style="color:#f92672">.</span>last_input):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>grad_weights[index] <span style="color:#f92672">+=</span> gradient_output[i]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Update the weights using the calculated gradient</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embedding_weights <span style="color:#f92672">-=</span> self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>grad_weights
</span></span></code></pre></div><hr>
<p>Each of 5 token is converted into 3 numbers.</p>
<p>This is called &ldquo;embedding&rdquo;.</p>
<p>Weights are used for embedding</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>d_embed <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Number of tokens:&#34;</span>, len(tokens))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Embedding dimention:&#34;</span>, d_embed)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>custom_embedding <span style="color:#f92672">=</span> TokenEmbedding(token_size, d_embed)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">weights</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">:&#34;</span>, custom_embedding<span style="color:#f92672">.</span>embedding_weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">vocas[0] ~ (tokenized)</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">tokens[0] =&gt; (embedding)</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">weights[0]=</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, custom_embedding<span style="color:#f92672">.</span>embedding_weights[<span style="color:#ae81ff">0</span>])
</span></span></code></pre></div><pre><code>Number of tokens: 5
Embedding dimention: 3

weights
: [[ 1.  2.  0.]
 [ 2.  2. -1.]
 [ 0.  0.  0.]
 [ 2.  1.  0.]
 [ 2. -1.  1.]]

vocas[0] ~ (tokenized)
tokens[0] =&gt; (embedding)
weights[0]=
 [1. 2. 0.]
</code></pre>
<hr>
<h2 id="3forwardpropagation">
  3.ForwardPropagation
  <a class="anchor" href="#3forwardpropagation">#</a>
</h2>
<p>Embedding Sentences
Using tokens, we can write sentences.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>input_ids <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>]])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;sentence0, voca0 voca1 </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">sentence1, voca2 voca2</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;input_ids size:s (n_batch,seq_len)&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;tokenized sentences:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, input_ids)
</span></span></code></pre></div><pre><code>sentence0, voca0 voca1 
sentence1, voca2 voca2

input_ids size:s (n_batch,seq_len)
tokenized sentences:
 [[0 1]
 [2 2]]
</code></pre>
<hr>
<p>We embed the sentences.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Pass the input through the custom embedding layer</span>
</span></span><span style="display:flex;"><span>embeddings <span style="color:#f92672">=</span> custom_embedding<span style="color:#f92672">.</span>forward(input_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">embedded sentences=</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">&#34;</span>, custom_embedding<span style="color:#f92672">.</span>embedding_weights[input_ids])
</span></span></code></pre></div><pre><code>embedded sentences=

 [[[ 1.  2.  0.]
  [ 2.  2. -1.]]

 [[ 0.  0.  0.]
  [ 0.  0.  0.]]]
</code></pre>
<hr>
<p>In equations, they are expressed followings :</p>
<p>sentence = S,<br>
input = X,<br>
weights = W,<br>
output = Y</p>

<link rel="stylesheet" href="/data_science_blog/katex/katex.min.css" />
<script defer src="/data_science_blog/katex/katex.min.js"></script>
<script defer src="/data_science_blog/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script><span>
  \[
\begin{aligned}
\text{X} &amp;= \sum_{i} \text{s}_i \bar{e}_i = \sum_{i,j} x_{ij} \bar{e}_i \bar{e}_j^T \\
\text{y}_i &amp;= \sum_{j,k} w_{x_{ij},k} \bar{e}_k^T \\
\text{Y} &amp;= \sum_{i} \text{y}_i \bar{e}_i = \sum_{i,j,k} w_{x_{ij},k} \bar{e}_i \bar{e}_k^T
\end{aligned}\]
</span>

<p>,where <span>
  \(\bar{e}\)
</span>
 refers to sentence axis, and <span>
  \(\bar{e^{T}}\)
</span>

refers to embedding vector axis</p>
<p>Check the shapes of input and output</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Input shape:&#34;</span>, input_ids<span style="color:#f92672">.</span>shape, <span style="color:#e6db74">&#34;= (n_sentences, len_sentences)&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Output shape:&#34;</span>, embeddings<span style="color:#f92672">.</span>shape, <span style="color:#e6db74">&#34;= (n_sentences, len_sentences, dim_embedding)&#34;</span>)
</span></span></code></pre></div><pre><code>Input shape: (2, 2) = (n_sentences, len_sentences)
Output shape: (2, 2, 3) = (n_sentences, len_sentences, dim_embedding)
</code></pre>
<hr>
<h2 id="4backpropagation">
  4.BackPropagation
  <a class="anchor" href="#4backpropagation">#</a>
</h2>
<p>By Back propagation, we adjust the weights.</p>
<p><span>
  \(\Delta_{Y_{ij}}\)
</span>
 is the error caused by <span>
  \(X_{ij}\)
</span>
.<br>
Remember that <span>
  \(X_{ij}\)
</span>
 is the index of the voca index.</p>
<p>Simply, all the errors caused by <span>
  \(X_{ij}\)
</span>
 whose value is 3,<br>
are actually caused by <span>
  \(W_3\)
</span>
.</p>
<p>Therefore,
$$\Delta_{W_{i}} = \sum_{j, k} \Delta_{Y_{jk}} \text{   for all j, k  such that } X_{jk} = i$$</p>
<p>Lets check our assumption</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Input_ids:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, input_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>error_tensor <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, size<span style="color:#f92672">=</span>embeddings<span style="color:#f92672">.</span>shape)<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>float64)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Error:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, error_tensor)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Expected dW[2] = </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">error_tensor[1, 0] + error_tensor[1, 1] =</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, error_tensor[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> error_tensor[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>custom_embedding<span style="color:#f92672">.</span>backward(error_tensor)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Result dW[2] = </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>custom_embedding<span style="color:#f92672">.</span>grad_weights[<span style="color:#ae81ff">2</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>Input_ids:
 [[0 1]
 [2 2]]

Error:
 [[[ 1.  2.  0.]
  [ 2.  2. -1.]]

 [[ 0.  0.  0.]
  [ 2.  1.  0.]]]

Expected dW[2] = 
error_tensor[1, 0] + error_tensor[1, 1] =
 [2. 1. 0.]

Result dW[2] = 
[2. 1. 0.]
</code></pre>
<hr>
<p>This error means, the embedding weights must be adjusted.</p>
<hr>
<h2 id="5-wrapping-up">
  5. Wrapping Up
  <a class="anchor" href="#5-wrapping-up">#</a>
</h2>
<ul>
<li>Token embedding effectively transforms tokens into meaningful numerical features, termed as embeddings.</li>
<li>Through training and iterative adjustments, this layer refines its representations, optimizing how tokens are embedded.</li>
<li>This transition from simple tokens to intricate vectors is pivotal in bridging the gap between machine computations and human language understanding.</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#contents">Contents</a></li>
    <li><a href="#1tokenizing">1.Tokenizing</a></li>
    <li><a href="#2embedding">2.Embedding</a></li>
    <li><a href="#3forwardpropagation">3.ForwardPropagation</a></li>
    <li><a href="#4backpropagation">4.BackPropagation</a></li>
    <li><a href="#5-wrapping-up">5. Wrapping Up</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>

















<link rel="stylesheet" href="https://crimson206.github.io/data_science_blog/css/styles.min.css">