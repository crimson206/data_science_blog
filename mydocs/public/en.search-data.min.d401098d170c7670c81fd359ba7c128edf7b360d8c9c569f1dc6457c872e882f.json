[{"id":0,"href":"/docs/data_science/","title":"Data Science","section":"Docs","content":""},{"id":1,"href":"/docs/data_science/sgd/introduction/","title":"Introduction","section":"Stochastic Gradient Descent","content":"\rIntroduction\r#\rThe fundamental concept behind gradient descent is to minimize a function \\(E = E(w)\\)\r. The gradient \\(\\nabla_\\mathbf{W} E\\)\rindicates the direction in which \\(E\\)\rincreases the fastest at a point, and the objective is to modify the values of \\(\\mathbf{W}\\)\riteratively to minimize \\(E\\)\r. This is achieved by updating the values of \\(w\\)\ras follows:\n\\[\r\\mathbf{W}^{\\prime}=\\mathbf{W}-\\eta \\nabla_\\mathbf{W} E \\]\rHere, \\(\\eta\\)\ris the learning rate. In the subsequent sections, we will derive the equations necessary for this process, while keeping the text succinct and to the point.\nForward Propagation\r#\rForward propagation refers to the computation of the output of a neural network given an input \\(\\mathbf{x}\\)\r. The equations for forward propagation are:\n\\[\r\\begin{aligned}\rE \u0026amp; = \\frac{1}{2}(t-y)^{2} \\\\\r\\mathbf{y} \u0026amp; = \\mathbf{W}_n \\mathbf{z}_{n-1} \u0026#43; \\mathbf{b}_n \\\\\r\\mathbf{z}_n \u0026amp; = h(\\mathbf{a}_n) \\\\\r\\mathbf{a}_n \u0026amp; = \\mathbf{W}_n \\mathbf{z}_{n-1} \u0026#43; \\mathbf{b}_n \\\\\r\\mathbf{a}_1 \u0026amp; = \\mathbf{W}_1 \\mathbf{x} \u0026#43; \\mathbf{b}_1 \\\\\r\\end{aligned}\r\\]\rWe can visualize the forward propagation process as follows:\nBackward Propagation\r#\rBackward propagation is used to compute the gradients of the error with respect to the weights and biases in a neural network. The equations for backward propagation are:\n\\[\r\\begin{aligned}\r\\mathbf{\\Delta y} \u0026amp;= \\nabla_{\\mathbf{y}} E = \\mathbf{t} - \\mathbf{y} \\\\\r\\mathbf{\\Delta a} \u0026amp;= \\nabla_{\\mathbf{a}} E = (\\nabla_{\\mathbf{y}} \\mathbf{a})^\\top \\mathbf{\\Delta y} \\\\\r\\mathbf{\\Delta z}_{n-1} \u0026amp;= \\mathbf{W}_{n}^{\\top} \\mathbf{\\Delta a}_{n} \\\\\r\\mathbf{\\Delta a}_{n-1} \u0026amp;= h\u0026#39;(\\mathbf{a}_{n-1}) \\odot \\mathbf{\\Delta z}_{n} \\\\\r\u0026amp;= h\u0026#39;(\\mathbf{a}_{n-1})\\mathbf{W}_{n}^{\\top} \\mathbf{\\Delta a}_{n}\r\\end{aligned}\r\\]\rwhere \\(\\Delta\\)\rdenotes the gradient of the error with respect to a variable. The visualization of backward propagation is shown below:\nSymbol Definitions\r#\rTable 1 lists the symbols used in this document, along with their names, alternative names, and alternative symbols.\nTable1 : Symbols, main names, alternative names, and alternative symbols\n"},{"id":2,"href":"/posts/about-page/","title":"About this Page","section":"Blog","content":"Welcome to my blog!\nThis blog is dedicated to sharing what I\u0026rsquo;ve learned, with a focus on data science. I am passionate about topics like machine learning, artificial intelligence, and autonomous programming.\nThe main purpose of this webpage is to provide educational content and insights into the fascinating field of data science. As I continue to deepen my understanding and knowledge, I will share valuable information with you.\nWhile the majority of the posts will revolve around data science, I may also explore other related topics. Additionally, I might occasionally share personal experiences or reflections on my journey in the field.\nPlease keep in mind that this blog is still in its early stages, so there may not be many posts available at the moment. However, I am dedicated to providing valuable content, and I encourage you to stay tuned for future updates.\nThank you for visiting, and I hope you enjoy your time here!\n"},{"id":3,"href":"/docs/data_science/sgd/","title":"Stochastic Gradient Descent","section":"Data Science","content":""},{"id":4,"href":"/posts/","title":"Blog","section":"Introduction","content":""},{"id":5,"href":"/docs/","title":"Docs","section":"Introduction","content":""}]