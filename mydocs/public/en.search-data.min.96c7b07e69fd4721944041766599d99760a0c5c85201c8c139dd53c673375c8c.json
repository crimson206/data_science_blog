[{"id":0,"href":"/data_science_blog/docs/data_science/","title":"Data Science","section":"Docs","content":""},{"id":1,"href":"/data_science_blog/posts/about-page/","title":"About this Page","section":"Blog","content":"Welcome to my blog!\nThis blog is dedicated to sharing what I\u0026rsquo;ve learned, with a focus on data science. I am passionate about topics like machine learning, artificial intelligence, and autonomous programming.\nThe main purpose of this webpage is to provide educational content and insights into the fascinating field of data science. As I continue to deepen my understanding and knowledge, I will share valuable information with you.\nWhile the majority of the posts will revolve around data science, I may also explore other related topics. Additionally, I might occasionally share personal experiences or reflections on my journey in the field.\nPlease keep in mind that this blog is still in its early stages, so there may not be many posts available at the moment. However, I am dedicated to providing valuable content, and I encourage you to stay tuned for future updates.\nThank you for visiting, and I hope you enjoy your time here!\n"},{"id":2,"href":"/data_science_blog/docs/data_science/nlp/","title":"Natural Language Process","section":"Data Science","content":""},{"id":3,"href":"/data_science_blog/docs/data_science/nlp/transformers/","title":"Transformers","section":"Natural Language Process","content":""},{"id":4,"href":"/data_science_blog/posts/","title":"Blog","section":"Introduction","content":""},{"id":5,"href":"/data_science_blog/docs/data_science/nlp/transformers/calculate_attention/","title":"Calculate Attention","section":"Transformers","content":"\rAttention Calculation\r#\rIn the following section, we\u0026rsquo;ll explore the core concept of attention calculation in Transformers, an integral mechanism that empowers them to capture relationships within sequences.\nDiscover how Queries, Keys, and Values interact to create this impactful feature.\nContents\r#\rQuery, Key and Value Attention Score Attention Probability Output Mask Fuction : calculate_attention Setup (Run this first to practice with codes) 1. Initializing Query, Key, and Value\r#\rIn Transformers, we deal with three main components: Query, Key, and Value.\nAll three share the shape \\(\r(n_{\\text{batch}}, n_{\\text{sequence}}, n_{\\text{embed}})\r\\)\r.\nConsider embeddings as defining features for each sequence.\nWhile random weights won\u0026rsquo;t yield insightful outputs, remember:\nQ, K, and V arise from a \u0026ldquo;single input\u0026rdquo; but via \u0026ldquo;distinct weights\u0026rdquo;. Through weight optimization, Transformers refine and produce significant outputs. np.random.seed(42) data = np.random.randn(1, 10, 4) # 1 batch, sequence length of 10, and embedding dimension of 4 W_q = np.random.randn(data.shape[-1], 4) # Weight for projecting data to Query W_k = np.random.randn(data.shape[-1], 4) # Weight for projecting data to Key W_v = np.random.randn(data.shape[-1], 4) # Weight for projecting data to Value query = np.matmul(data, W_q) key = np.matmul(data, W_k) value = np.matmul(data, W_v) # Similarity injection. query[0][2] = [2, -2, 2, -2] key[0][5] = [2, -2, 2, -2] d_k = key.shape[-1] attention_score = np.matmul(query, np.transpose(key, (0, 2, 1))) # Q x K^T, (n_batch, seq_len, seq_len) # a precaution to handle the potential problem of having large dot products attention_score = attention_score / np.sqrt(d_k) 2. Attention Score\r#\rEach sequence is represented by an embedding vector.\nThe attention score between any sequence pair from the Query and Key is derived from their dot product. This means both similarity and magnitude play roles.\nNote the high score at our point of observation.\ndraw_mat_multi( query[0], key[0].T, attention_score[0], [\u0026#34;Fearues\u0026#34;, \u0026#34;Sequences\u0026#34;, \u0026#34;Query\u0026#34;], [\u0026#34;Sequences\u0026#34;, \u0026#34;Fearues\u0026#34;, \u0026#34;Key.T\u0026#34;], [\u0026#34;Key Sequences\u0026#34;, \u0026#34;Query Sequences\u0026#34;, \u0026#34;Attention Score\u0026#34;], pairs=np.array([[2, 5], [2, 3]]), figsize=[5,5], width_ratios=[2,5], height_ratios=[2,5], s=50, ) 3. Attention Probability\r#\rUsing the softmax function, attention scores are normalized across key sequences for each query.\nRecall: Q and K originate from the same input.\nAttention probabilities depict the unique influence each sequence receives from other sequences.\ndef softmax(x): exp_x = np.exp(x) sum_exp_x = np.sum(exp_x, axis=-1, keepdims=True) return exp_x / sum_exp_x attention_prob = np.apply_along_axis(softmax, -1, attention_score) draw_mat_horizon_transfer( attention_score[0], attention_prob[0], info_a=[\u0026#34;Key Sequences\u0026#34;, \u0026#34;Query Sequences\u0026#34;, \u0026#34;Attention Score\u0026#34;], info_b=[\u0026#34;Key Sequences\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Attention Probabilities\u0026#34;], ) 4. Output\r#\rQ and K dictate the information flow proportion, while V carries the information itself.\nEvery feature_i in a sequence is a linear blend of feature_i\u0026rsquo;s from all sequences.\ndraw_mat_multi( attention_prob[0], value[0], output[0], [\u0026#34;Key Sequences\u0026#34;, \u0026#34;Query Sequences\u0026#34;, \u0026#34;Attention Probabilities\u0026#34;], [\u0026#34;Fearues\u0026#34;, \u0026#34;Sequences\u0026#34;, \u0026#34;Value\u0026#34;], [\u0026#34;Sequences\u0026#34;,\u0026#34;Fearues\u0026#34;, \u0026#34;Output\u0026#34;], pairs=np.array([[2, 3], [2, 3]]), figsize=[5,5], width_ratios=[5,2], height_ratios=[1,1], ) 5. Masked Attention Probabilities\r#\rTo make layers prioritize neighboring word contexts, use the following mask.\nOf course, masks can be tailored as needed.\nseq_len = 10 mask = np.zeros((seq_len, seq_len)) for i in range(seq_len): mask[i, max(0, i-1):min(seq_len, i+2)] = 1 # Expand dimensions for batch size mask = np.expand_dims(mask, 0) sns.heatmap(mask[0], cmap=\u0026#39;RdBu\u0026#39;, center=0) plt.show() We computed the probabilities again using the masked scores. Note that, the high probability in the point (2,5) disappeared.\nmasked_attention_score = np.where(mask == 0, -1e9, attention_score) masked_attention_prob = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x), axis=-1), -1, masked_attention_score) draw_mat_horizon_transfer( masked_attention_score[0], masked_attention_prob[0], info_a=[\u0026#34;Key Sequences\u0026#34;, \u0026#34;Query Sequences\u0026#34;, \u0026#34;Attention Score\u0026#34;], info_b=[\u0026#34;Key Sequences\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Attention Probabilities\u0026#34;], ) 6. Function for Attention computation\r#\rdef calculate_attention(query, key, value, mask=None): # query, key, value: (n_batch, seq_len, d_k) # mask: (n_batch, seq_len, seq_len) d_k = key.shape[-1] attention_score = np.matmul(query, np.transpose(key, (0, 2, 1))) # Q x K^T, (n_batch, seq_len, seq_len) attention_score = attention_score / np.sqrt(d_k) if mask is not None: attention_score = np.where(mask == 0, -1e9, attention_score) attention_prob = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x), axis=-1), -1, attention_score) out = np.matmul(attention_prob, value) # (n_batch, seq_len, d_k) return out 7.Setup (Run this first to practice with codes)\r#\rimport numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import seaborn as sns def draw_mat_multi(mat_a, mat_b, mat_ab, info_a=None, info_b=None, info_ab=None, pairs=np.array([[0, 0],[1, 1]]), figsize=(5, 5), width_ratios=[1, 1], height_ratios=[1, 1], linewidth=2, s=100): # Adjusting pairs for visualization if pairs is not None: pairs = pairs + 0.5 plt.figure(figsize=figsize) gs = gridspec.GridSpec(2, 2, width_ratios=width_ratios, height_ratios=height_ratios) # Attention Probabilities visualization ax0 = plt.subplot(gs[1, 0]) sns.heatmap(mat_a, cmap=\u0026#39;RdBu\u0026#39;, center=0, cbar=True) if pairs is not None: ax0.axhline(y=pairs[0][0], color=\u0026#39;blue\u0026#39;, linewidth=linewidth) ax0.axhline(y=pairs[1][0], color=\u0026#39;red\u0026#39;, linewidth=linewidth) if info_a is not None: ax0.set_xlabel(info_a[0]) ax0.set_ylabel(info_a[1]) ax0.set_title(info_a[2]) # Value matrix visualization ax1 = plt.subplot(gs[0, 1]) sns.heatmap(mat_b, cmap=\u0026#39;RdBu\u0026#39;, center=0, cbar=True) if pairs is not None: ax1.axvline(x=pairs[0][1], color=\u0026#39;blue\u0026#39;, linewidth=linewidth) ax1.axvline(x=pairs[1][1], color=\u0026#39;red\u0026#39;, linewidth=linewidth) if info_b is not None: ax1.set_xlabel(info_b[0]) ax1.set_ylabel(info_b[1]) ax1.set_title(info_b[2]) # Output of attention mechanism visualization ax2 = plt.subplot(gs[1, 1]) sns.heatmap(mat_ab, cmap=\u0026#39;RdBu\u0026#39;, center=0, cbar=True) if pairs is not None: ax2.scatter(pairs[0][1], pairs[0][0], color=\u0026#39;blue\u0026#39;, s=s) ax2.scatter(pairs[1][1], pairs[1][0], color=\u0026#39;red\u0026#39;, s=s) if info_ab is not None: ax2.set_xlabel(info_ab[0]) ax2.set_ylabel(info_ab[1]) ax2.set_title(info_ab[2]) plt.tight_layout() plt.show() def draw_mat_horizon_transfer(mat_a, mat_b, info_a, info_b): # Create a gridspec for matrix multiplication visualization fig = plt.figure(figsize=(6, 3)) gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1]) # Attention Probabilities visualization ax0 = plt.subplot(gs[0, 0]) sns.heatmap(mat_a, cmap=\u0026#39;RdBu\u0026#39;, center=0, cbar=True) ax0.axhline(y=2.5, color=\u0026#39;blue\u0026#39;, linewidth=2) if info_a is not None: ax0.set_xlabel(info_a[0]) ax0.set_ylabel(info_a[1]) ax0.set_title(info_a[2]) # Value matrix visualization ax1 = plt.subplot(gs[0, 1]) sns.heatmap(mat_b, cmap=\u0026#39;RdBu\u0026#39;, center=0, cbar=True) ax1.axhline(y=2.5, color=\u0026#39;blue\u0026#39;, linewidth=2) if info_b is not None: ax1.set_xlabel(info_b[0]) ax1.set_ylabel(info_b[1]) ax1.set_title(info_b[2]) "},{"id":6,"href":"/data_science_blog/docs/data_science/nlp/transformers/position_embedding/","title":"Position Embedding","section":"Transformers","content":"\rPositional Encoding\r#\rPositional encoding is a technique used in Transformers to provide sequence-awareness.\nBy adding unique patterns to token embeddings based on their position in a sequence, the Transformer model can discern token order.\nContents\r#\rPositionalEncoding ArbitraryInput PosionalEncoding PatternAnalysis PositionPrediction Post-Processing? Summary 0. Layer Implementation\r#\rIn this article, we will the class,\nimport numpy as np import math import matplotlib.pyplot as plt class PositionalEncoding: def __init__(self, d_embed, max_len=400): encoding = np.zeros((max_len, d_embed)) position = np.arange(0, max_len).reshape(-1, 1) div_term = np.exp(np.arange(0, d_embed, 2) * -(np.log(10000.0) / d_embed)) encoding[:, 0::2] = np.sin(position * div_term) encoding[:, 1::2] = np.cos(position * div_term) self.encoding = encoding.reshape(1, max_len, d_embed) self._pos_embed = None def forward(self, x): seq_len = x.shape[1] self._pos_embed = self.encoding[:, :seq_len, :] out = x + self._pos_embed return out 1. Arbitrary input\r#\rLets prepare the input data and visualize.\nn_batch = 2 n_words = 300 d_embed = 20 pos_encoding = PositionalEncoding(d_embed) np.random.seed(42) x = np.random.randn(n_batch, n_words, d_embed) / 2 # Plotting the positionally encoded input plt.figure(figsize=(5, 3)) plt.pcolormesh(x[0].squeeze().T, cmap=\u0026#39;viridis\u0026#39;) plt.title(\u0026#39;Input\u0026#39;) plt.colorbar() plt.show() 2. Visualization of encoded information and output\r#\rback up the input data, and encode the positional information\nx_backup = x output = pos_encoding.forward(x) pos_encoding_array = pos_encoding._pos_embed[0] # Plotting the positional encoding plt.figure(figsize=(5, 3)) plt.pcolormesh(pos_encoding_array.T, cmap=\u0026#39;viridis\u0026#39;) plt.title(\u0026#39;Positional Encoding\u0026#39;) plt.colorbar() plt.axvline(x=50, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # First vertical dotted line at position 5 plt.axvline(x=100, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # Second vertical dotted line at position 10 plt.xlabel(\u0026#39;Axis with patterns we focus on\u0026#39;) # Highlighting the axis of interest plt.show() # Plotting the positionally encoded input plt.figure(figsize=(5, 3)) plt.pcolormesh(output[0].squeeze().T, cmap=\u0026#39;viridis\u0026#39;) plt.title(\u0026#39;Positionally Encoded output\u0026#39;) plt.colorbar() plt.axvline(x=50, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # First vertical dotted line at position 5 plt.axvline(x=100, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # Second vertical dotted line at position 10 plt.xlabel(\u0026#39;Axis with patterns we focus on\u0026#39;) # Highlighting the axis of interest plt.show() The visualized output is from the first batch.\nPlease note that, 300 is the number of words.\nThe positional encoding gives unique patterns to each word.\nCompare the patterns highlighted by the dot red lines.\n3. Pattern analysis\r#\rIf two pattenrs are the same, their dot product give large similarity score.\nSee the simplest example.\n[1, 0, 1] * [1, 0, 1] = [1, 0, 1] =\u0026gt; 2 [0, 1, 0] * [1, 0, 1] = [0, 0, 0] =\u0026gt; 0\nUsing this characteristic, we predict the positions of words.\npos30_predictor = pos_encoding_array[30] # the 30th positional encoding similarity_scores = np.dot(pos_encoding_array, pos30_predictor) # Plotting the similarity scores plt.figure(figsize=(5, 3)) plt.plot(similarity_scores) plt.title(\u0026#39;Self Similarity with 30th Position Encoding\u0026#39;) plt.axvline(x=30, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # First vertical dotted line at position 5 plt.xlabel(\u0026#39;Position\u0026#39;) plt.ylabel(\u0026#39;Similarity Score\u0026#39;) plt.grid(True) plt.show() As positional encodings are designed to be unique for each position,\nwe see the highest similarity score at the 30th position (as it is compared with itself),\nand lower similarity scores for other positions.\nDo the same with the encoded output\nsimilarity_scores = np.dot(output[0], pos30_predictor) # Find the index of the highest similarity score max_index = np.argmax(similarity_scores) # Plotting the similarity scores plt.figure(figsize=(5, 3)) plt.plot(similarity_scores) plt.title(\u0026#39;Output Similarity with 30th Position Encoding\u0026#39;) plt.axvline(x=30, color=\u0026#39;red\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=\u0026#39;30th position\u0026#39;) # Vertical line at position 30 plt.axvline(x=max_index, color=\u0026#39;blue\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=\u0026#39;Max similarity position\u0026#39;) # Vertical line at the position of highest similarity plt.xlabel(\u0026#39;Position\u0026#39;) plt.ylabel(\u0026#39;Similarity Score\u0026#39;) plt.legend() # Adding a legend to distinguish between the lines plt.grid(True) plt.show() 4. Position information recovery\r#\rThe word at position 60 was predicted to be at position 30.\nHow good would this prediction be in general?\nFor each word, we will apply all positional encoding patterns, and predict its position as the one that gives the highest similarity score.\nHere\u0026rsquo;s how we can do it:\nCalculate the similarity scores between each word and every positional encoding pattern. For each word, find the position that gives the highest similarity score.\n# Compute the similarity scores matrix similarity_scores_matrix = np.dot(output[0], pos_encoding_array.T) # For each word, predict its position as the one that gives the highest similarity score predicted_positions = np.argmax(similarity_scores_matrix, axis=1) plt.figure(figsize=(5, 3)) plt.plot(predicted_positions, \u0026#39;bo-\u0026#39;, linewidth=0.5, markersize=2) plt.title(\u0026#39;Predicted Positions for Each Word\u0026#39;) plt.xlabel(\u0026#39;True Position\u0026#39;) plt.ylabel(\u0026#39;Predicted Position\u0026#39;) plt.grid(True) plt.show() 5. Is There a Need for Post-Processing?\r#\rIndeed.\nDeep learning models, like Transformers, inherently recognize and interpret patterns.\nThrough training, they will independently discern and effectively utilize these positional encodings.\n6. Summary\r#\rPositional encoding is essential for embedding sequence order into Transformers. Unique patterns are added to token embeddings, enabling the model to determine token positions within a sequence. "},{"id":7,"href":"/data_science_blog/docs/data_science/nlp/transformers/token_embedding/","title":"Token Embedding","section":"Transformers","content":"\rToken Embedding\r#\rToken embeddings provide a numerical representation for words, capturing their semantic meaning and allowing machines to process and understand text.\nThis article delves into the mechanics of token embeddings, from tokenization to weight adjustments in neural networks.\nContents\r#\rTokenizing Embedding Forward Backward WrappingUp 1.Tokenizing\r#\rAssume that We have vocabulary items (or tokens)\ntoken_size = 5 vocas = [f\u0026#34;voca{i}\u0026#34; for i in range(token_size)] tokens = [i for i in range(token_size)] print(\u0026#34;\\nvocas:\\n\u0026#34;, vocas) print(\u0026#34;\\ntokenized_vocas:\\n\u0026#34;, tokens) vocas:\r['voca0', 'voca1', 'voca2', 'voca3', 'voca4']\rtokenized_vocas:\r[0, 1, 2, 3, 4]\r2.Embedding\r#\rA Embedding class is defined. All the things will be explained.\nSkip understanding for now.\nimport numpy as np class TokenEmbedding: def __init__(self, vocab_size, d_embed, learning_rate=0.1): self.vocab_size = vocab_size self.d_embed = d_embed self.learning_rate = learning_rate # Initialize the weight matrix randomly np.random.seed(42) self.embedding_weights = np.random.randint(-2, 3, size=(vocab_size, d_embed)).astype(np.float64) # Save the last input for backward pass self.last_input = None self.grad_weights = None def forward(self, x): # Save the input for the backward pass self.last_input = x # Use the input indices to look up the corresponding vectors in the weight matrix return self.embedding_weights[x] def backward(self, gradient_output): # Initialize a gradient matrix for the weights, filled with zeros self.grad_weights = np.zeros_like(self.embedding_weights) # Accumulate the gradients at the positions corresponding to the input indices for i, index in enumerate(self.last_input): self.grad_weights[index] += gradient_output[i] # Update the weights using the calculated gradient self.embedding_weights -= self.learning_rate * self.grad_weights Each of 5 token is converted into 3 numbers.\nThis is called \u0026ldquo;embedding\u0026rdquo;.\nWeights are used for embedding\nd_embed = 3 print(\u0026#34;Number of tokens:\u0026#34;, len(tokens)) print(\u0026#34;Embedding dimention:\u0026#34;, d_embed) custom_embedding = TokenEmbedding(token_size, d_embed) print(\u0026#34;\\nweights\\n:\u0026#34;, custom_embedding.embedding_weights) print(\u0026#34;\\nvocas[0] ~ (tokenized)\\ntokens[0] =\u0026gt; (embedding)\\nweights[0]=\\n\u0026#34;, custom_embedding.embedding_weights[0]) Number of tokens: 5\rEmbedding dimention: 3\rweights\r: [[ 1. 2. 0.]\r[ 2. 2. -1.]\r[ 0. 0. 0.]\r[ 2. 1. 0.]\r[ 2. -1. 1.]]\rvocas[0] ~ (tokenized)\rtokens[0] =\u0026gt; (embedding)\rweights[0]=\r[1. 2. 0.]\r3.ForwardPropagation\r#\rEmbedding Sentences Using tokens, we can write sentences.\ninput_ids = np.array([[0, 1], [2, 2]]) print(\u0026#34;sentence0, voca0 voca1 \\nsentence1, voca2 voca2\\n\u0026#34;) print(\u0026#34;input_ids size:s (n_batch,seq_len)\u0026#34;) print(\u0026#34;tokenized sentences:\\n\u0026#34;, input_ids) sentence0, voca0 voca1 sentence1, voca2 voca2\rinput_ids size:s (n_batch,seq_len)\rtokenized sentences:\r[[0 1]\r[2 2]]\rWe embed the sentences.\n# Pass the input through the custom embedding layer embeddings = custom_embedding.forward(input_ids) print(\u0026#34;\\nembedded sentences=\\n\\n\u0026#34;, custom_embedding.embedding_weights[input_ids]) embedded sentences=\r[[[ 1. 2. 0.]\r[ 2. 2. -1.]]\r[[ 0. 0. 0.]\r[ 0. 0. 0.]]]\rIn equations, they are expressed followings :\nsentence = S,\ninput = X,\nweights = W,\noutput = Y\n\\[\r\\begin{aligned}\r\\text{X} \u0026amp;= \\sum_{i} \\text{s}_i \\bar{e}_i = \\sum_{i,j} x_{ij} \\bar{e}_i \\bar{e}_j^T \\\\\r\\text{y}_i \u0026amp;= \\sum_{j,k} w_{x_{ij},k} \\bar{e}_k^T \\\\\r\\text{Y} \u0026amp;= \\sum_{i} \\text{y}_i \\bar{e}_i = \\sum_{i,j,k} w_{x_{ij},k} \\bar{e}_i \\bar{e}_k^T\r\\end{aligned}\r\\]\r,where \\(\\bar{e}\\)\rrefers to sentence axis, and \\(\\bar{e^{T}}\\)\rrefers to embedding vector axis\nCheck the shapes of input and output\nprint(\u0026#34;Input shape:\u0026#34;, input_ids.shape, \u0026#34;= (n_sentences, len_sentences)\u0026#34;) print(\u0026#34;Output shape:\u0026#34;, embeddings.shape, \u0026#34;= (n_sentences, len_sentences, dim_embedding)\u0026#34;) Input shape: (2, 2) = (n_sentences, len_sentences)\rOutput shape: (2, 2, 3) = (n_sentences, len_sentences, dim_embedding)\r4.BackPropagation\r#\rBy Back propagation, we adjust the weights.\n\\(\\Delta_{Y_{ij}}\\)\ris the error caused by \\(X_{ij}\\)\r.\nRemember that \\(X_{ij}\\)\ris the index of the voca index.\nSimply, all the errors caused by \\(X_{ij}\\)\rwhose value is 3,\nare actually caused by \\(W_3\\)\r.\nTherefore, $$\\Delta_{W_{i}} = \\sum_{j, k} \\Delta_{Y_{jk}} \\text{ for all j, k such that } X_{jk} = i$$\nLets check our assumption\nprint(\u0026#34;Input_ids:\\n\u0026#34;, input_ids) np.random.seed(42) error_tensor = np.random.randint(-2, 3, size=embeddings.shape).astype(np.float64) print(\u0026#34;\\nError:\\n\u0026#34;, error_tensor) print(f\u0026#34;\\nExpected dW[2] = \\nerror_tensor[1, 0] + error_tensor[1, 1] =\\n\u0026#34;, error_tensor[1, 0] + error_tensor[1, 1]) custom_embedding.backward(error_tensor) print(f\u0026#34;\\nResult dW[2] = \\n{custom_embedding.grad_weights[2]}\u0026#34;) Input_ids:\r[[0 1]\r[2 2]]\rError:\r[[[ 1. 2. 0.]\r[ 2. 2. -1.]]\r[[ 0. 0. 0.]\r[ 2. 1. 0.]]]\rExpected dW[2] = error_tensor[1, 0] + error_tensor[1, 1] =\r[2. 1. 0.]\rResult dW[2] = [2. 1. 0.]\rThis error means, the embedding weights must be adjusted.\n5. Wrapping Up\r#\rToken embedding effectively transforms tokens into meaningful numerical features, termed as embeddings. Through training and iterative adjustments, this layer refines its representations, optimizing how tokens are embedded. This transition from simple tokens to intricate vectors is pivotal in bridging the gap between machine computations and human language understanding. "},{"id":8,"href":"/data_science_blog/docs/","title":"Docs","section":"Introduction","content":""}]