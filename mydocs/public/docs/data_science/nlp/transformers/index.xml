<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformers on Discovering Data Science with Crimson</title>
    <link>https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/</link>
    <description>Recent content in Transformers on Discovering Data Science with Crimson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/calculate_attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/calculate_attention/</guid>
      <description>Attention Calculation#In the following section, we&amp;rsquo;ll explore the core concept of attention calculation in Transformers, an integral mechanism that empowers them to capture relationships within sequences.
Discover how Queries, Keys, and Values interact to create this impactful feature.
Contents#Query, Key and Value Attention Score Attention Probability Output Mask Fuction : calculate_attention Setup (Run this first to practice with codes) 1. Initializing Query, Key, and Value#In Transformers, we deal with three main components: Query, Key, and Value.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/position_embedding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/position_embedding/</guid>
      <description>Positional Encoding#Positional encoding is a technique used in Transformers to provide sequence-awareness.
By adding unique patterns to token embeddings based on their position in a sequence, the Transformer model can discern token order.
Contents#PositionalEncoding ArbitraryInput PosionalEncoding PatternAnalysis PositionPrediction Post-Processing? Summary 0. Layer Implementation#In this article, we will the class,
import numpy as np import math import matplotlib.pyplot as plt class PositionalEncoding: def __init__(self, d_embed, max_len=400): encoding = np.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/token_embedding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/token_embedding/</guid>
      <description>Token Embedding#Token embeddings provide a numerical representation for words, capturing their semantic meaning and allowing machines to process and understand text.
This article delves into the mechanics of token embeddings, from tokenization to weight adjustments in neural networks.
Contents#Tokenizing Embedding Forward Backward WrappingUp 1.Tokenizing#Assume that We have vocabulary items (or tokens)
token_size = 5 vocas = [f&amp;#34;voca{i}&amp;#34; for i in range(token_size)] tokens = [i for i in range(token_size)] print(&amp;#34;\nvocas:\n&amp;#34;, vocas) print(&amp;#34;\ntokenized_vocas:\n&amp;#34;, tokens) vocas:[&#39;voca0&#39;, &#39;voca1&#39;, &#39;voca2&#39;, &#39;voca3&#39;, &#39;voca4&#39;]tokenized_vocas:[0, 1, 2, 3, 4]2.</description>
    </item>
    
  </channel>
</rss>
