<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Positional Encoding#Positional encoding is a technique used in Transformers to provide sequence-awareness.
By adding unique patterns to token embeddings based on their position in a sequence, the Transformer model can discern token order.
Contents#PositionalEncoding ArbitraryInput PosionalEncoding PatternAnalysis PositionPrediction Post-Processing? Summary 0. Layer Implementation#In this article, we will the class,
import numpy as np import math import matplotlib.pyplot as plt class PositionalEncoding: def __init__(self, d_embed, max_len=400): encoding = np.">
<meta name="theme-color" content="#FFFFFF">
<meta name="color-scheme" content="light dark"><meta property="og:title" content="" />
<meta property="og:description" content="Positional Encoding#Positional encoding is a technique used in Transformers to provide sequence-awareness.
By adding unique patterns to token embeddings based on their position in a sequence, the Transformer model can discern token order.
Contents#PositionalEncoding ArbitraryInput PosionalEncoding PatternAnalysis PositionPrediction Post-Processing? Summary 0. Layer Implementation#In this article, we will the class,
import numpy as np import math import matplotlib.pyplot as plt class PositionalEncoding: def __init__(self, d_embed, max_len=400): encoding = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://crimson206.github.io/data_science_blog/docs/data_science/nlp/transformers/position_embedding/" /><meta property="article:section" content="docs" />


<title>Position Embedding | Discovering Data Science with Crimson</title>
<link rel="manifest" href="/data_science_blog/manifest.json">
<link rel="icon" href="/data_science_blog/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/data_science_blog/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css" integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz&#43;OQteg=" crossorigin="anonymous">
  <script defer src="/data_science_blog/flexsearch.min.js"></script>
  <script defer src="/data_science_blog/en.search.min.a2040b3bb4198ad5c945338be7050307a399d764565c1e3bb5d4f76954ca9508.js" integrity="sha256-ogQLO7QZitXJRTOL5wUDB6OZ12RWXB47tdT3aVTKlQg=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/data_science_blog/"><span>Discovering Data Science with Crimson</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="" aria-label="" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>












  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <span>Data Science</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b2f022745f8088cd643ee6d2da035e6b" class="toggle" checked />
    <label for="section-b2f022745f8088cd643ee6d2da035e6b" class="flex justify-between">
      <a role="button" class="">Natural Language Process</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d06abac363844c6f23395806fd3fb26d" class="toggle" checked />
    <label for="section-d06abac363844c6f23395806fd3fb26d" class="flex justify-between">
      <a role="button" class="">Transformers</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/data_science_blog/docs/data_science/nlp/transformers/calculate_attention/" class="">Calculate Attention</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/data_science_blog/docs/data_science/nlp/transformers/position_embedding/" class="active">Position Embedding</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/data_science_blog/docs/data_science/nlp/transformers/token_embedding/" class="">Token Embedding</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>











  
<ul>
  
  <li>
    <a href="/data_science_blog/posts/"  >
        Blog
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/data_science_blog/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Position Embedding</strong>

  <label for="toc-control">
    
    <img src="/data_science_blog/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#contents">Contents</a></li>
    <li><a href="#0-layer-implementation">0. Layer Implementation</a></li>
    <li><a href="#1-arbitrary-input">1. Arbitrary input</a></li>
    <li><a href="#2-visualization-of-encoded-information-and-output">2. Visualization of encoded information and output</a></li>
    <li><a href="#3-pattern-analysis">3. Pattern analysis</a></li>
    <li><a href="#4-position-information-recovery">4. Position information recovery</a></li>
    <li><a href="#5-is-there-a-need-for-post-processing">5. Is There a Need for Post-Processing?</a></li>
    <li><a href="#6-summary">6. Summary</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="positional-encoding">
  Positional Encoding
  <a class="anchor" href="#positional-encoding">#</a>
</h1>
<p>Positional encoding is a technique used in Transformers to provide sequence-awareness.</p>
<p>By adding unique patterns to token embeddings based on their position in a sequence, the Transformer model can discern token order.</p>
<h2 id="contents">
  Contents
  <a class="anchor" href="#contents">#</a>
</h2>
<ul>
<li><a href="#0-layer-implementation">PositionalEncoding</a></li>
<li><a href="#1-arbitrary-input">ArbitraryInput</a></li>
<li><a href="#2-visualization-of-encoded-information-and-output">PosionalEncoding</a></li>
<li><a href="#3-pattern-analysis">PatternAnalysis</a></li>
<li><a href="#4-position-information-recovery">PositionPrediction</a></li>
<li><a href="#5-there-is-the-post-processing">Post-Processing?</a></li>
<li><a href="#6-summary">Summary</a></li>
</ul>
<hr>
<h2 id="0-layer-implementation">
  0. Layer Implementation
  <a class="anchor" href="#0-layer-implementation">#</a>
</h2>
<p>In this article, we will the class,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PositionalEncoding</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, d_embed, max_len<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>):
</span></span><span style="display:flex;"><span>        encoding <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((max_len, d_embed))
</span></span><span style="display:flex;"><span>        position <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, max_len)<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        div_term <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, d_embed, <span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> <span style="color:#f92672">-</span>(np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">10000.0</span>) <span style="color:#f92672">/</span> d_embed))
</span></span><span style="display:flex;"><span>        encoding[:, <span style="color:#ae81ff">0</span>::<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sin(position <span style="color:#f92672">*</span> div_term)
</span></span><span style="display:flex;"><span>        encoding[:, <span style="color:#ae81ff">1</span>::<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cos(position <span style="color:#f92672">*</span> div_term)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>encoding <span style="color:#f92672">=</span> encoding<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, max_len, d_embed)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_pos_embed <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        seq_len <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_pos_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoding[:, :seq_len, :]
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>_pos_embed
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><hr>
<h2 id="1-arbitrary-input">
  1. Arbitrary input
  <a class="anchor" href="#1-arbitrary-input">#</a>
</h2>
<p>Lets prepare the input data and visualize.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>n_batch <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>n_words <span style="color:#f92672">=</span> <span style="color:#ae81ff">300</span>
</span></span><span style="display:flex;"><span>d_embed <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pos_encoding <span style="color:#f92672">=</span> PositionalEncoding(d_embed)
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(n_batch, n_words, d_embed) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting the positionally encoded input</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>pcolormesh(x[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>T, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;viridis&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Input&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>colorbar()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><div class="centered-image">
    <img src="/data_science_blog/images/nlp/transformers/position_embedding_files/position_embedding_3_0.png" alt="" />
    <p></p>
</div>

<h2 id="2-visualization-of-encoded-information-and-output">
  2. Visualization of encoded information and output
  <a class="anchor" href="#2-visualization-of-encoded-information-and-output">#</a>
</h2>
<p>back up the input data, and encode the positional information</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_backup <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> pos_encoding<span style="color:#f92672">.</span>forward(x)
</span></span><span style="display:flex;"><span>pos_encoding_array <span style="color:#f92672">=</span> pos_encoding<span style="color:#f92672">.</span>_pos_embed[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting the positional encoding</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>pcolormesh(pos_encoding_array<span style="color:#f92672">.</span>T, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;viridis&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Positional Encoding&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>colorbar()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)  <span style="color:#75715e"># First vertical dotted line at position 5</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)  <span style="color:#75715e"># Second vertical dotted line at position 10</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Axis with patterns we focus on&#39;</span>)  <span style="color:#75715e"># Highlighting the axis of interest</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting the positionally encoded input</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>pcolormesh(output[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>T, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;viridis&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Positionally Encoded output&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>colorbar()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)  <span style="color:#75715e"># First vertical dotted line at position 5</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)  <span style="color:#75715e"># Second vertical dotted line at position 10</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Axis with patterns we focus on&#39;</span>)  <span style="color:#75715e"># Highlighting the axis of interest</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><div class="centered-image">
    <img src="/data_science_blog/images/nlp/transformers/position_embedding_files/position_embedding_5_0.png" alt="" />
    <p></p>
</div>

<div class="centered-image">
    <img src="/data_science_blog/images/nlp/transformers/position_embedding_files/position_embedding_5_1.png" alt="" />
    <p></p>
</div>

<hr>
<p>The visualized output is from the first batch.</p>
<p>Please note that, 300 is the number of words.</p>
<p>The positional encoding gives unique patterns to each word.</p>
<p>Compare the patterns highlighted by the dot red lines.</p>
<hr>
<h2 id="3-pattern-analysis">
  3. Pattern analysis
  <a class="anchor" href="#3-pattern-analysis">#</a>
</h2>
<p>If two pattenrs are the same, their dot product give large similarity score.</p>
<p>See the simplest example.</p>
<p>[1, 0, 1] * [1, 0, 1] = [1, 0, 1] =&gt; 2
[0, 1, 0] * [1, 0, 1] = [0, 0, 0] =&gt; 0</p>
<p>Using this characteristic, we predict the positions of words.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pos30_predictor <span style="color:#f92672">=</span> pos_encoding_array[<span style="color:#ae81ff">30</span>]  <span style="color:#75715e"># the 30th positional encoding</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>similarity_scores <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(pos_encoding_array, pos30_predictor)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting the similarity scores</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(similarity_scores)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Self Similarity with 30th Position Encoding&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>)  <span style="color:#75715e"># First vertical dotted line at position 5</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Position&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Similarity Score&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><div class="centered-image">
    <img src="/data_science_blog/images/nlp/transformers/position_embedding_files/position_embedding_8_0.png" alt="" />
    <p></p>
</div>

<hr>
<p>As positional encodings are designed to be unique for each position,</p>
<p>we see the highest similarity score at the 30th position (as it is compared with itself),</p>
<p>and lower similarity scores for other positions.</p>
<p>Do the same with the encoded output</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>similarity_scores <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(output[<span style="color:#ae81ff">0</span>], pos30_predictor)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Find the index of the highest similarity score</span>
</span></span><span style="display:flex;"><span>max_index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(similarity_scores)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting the similarity scores</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(similarity_scores)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Output Similarity with 30th Position Encoding&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;30th position&#39;</span>)  <span style="color:#75715e"># Vertical line at position 30</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span>max_index, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Max similarity position&#39;</span>)  <span style="color:#75715e"># Vertical line at the position of highest similarity</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Position&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Similarity Score&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  <span style="color:#75715e"># Adding a legend to distinguish between the lines</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><div class="centered-image">
    <img src="/data_science_blog/images/nlp/transformers/position_embedding_files/position_embedding_10_0.png" alt="" />
    <p></p>
</div>

<hr>
<h2 id="4-position-information-recovery">
  4. Position information recovery
  <a class="anchor" href="#4-position-information-recovery">#</a>
</h2>
<p>The word at position 60 was predicted to be at position 30.</p>
<p>How good would this prediction be in general?</p>
<p>For each word, we will apply all positional encoding patterns, and predict its position as the one that gives the highest similarity score.</p>
<p>Here&rsquo;s how we can do it:</p>
<p>Calculate the similarity scores between each word and every positional encoding pattern.
For each word, find the position that gives the highest similarity score.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Compute the similarity scores matrix</span>
</span></span><span style="display:flex;"><span>similarity_scores_matrix <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(output[<span style="color:#ae81ff">0</span>], pos_encoding_array<span style="color:#f92672">.</span>T)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># For each word, predict its position as the one that gives the highest similarity score</span>
</span></span><span style="display:flex;"><span>predicted_positions <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(similarity_scores_matrix, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(predicted_positions, <span style="color:#e6db74">&#39;bo-&#39;</span>, linewidth<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, markersize<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Predicted Positions for Each Word&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;True Position&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Predicted Position&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><div class="centered-image">
    <img src="/data_science_blog/images/nlp/transformers/position_embedding_files/position_embedding_12_0.png" alt="" />
    <p></p>
</div>

<hr>
<h2 id="5-is-there-a-need-for-post-processing">
  5. Is There a Need for Post-Processing?
  <a class="anchor" href="#5-is-there-a-need-for-post-processing">#</a>
</h2>
<p>Indeed.</p>
<p>Deep learning models, like Transformers, inherently recognize and interpret patterns.</p>
<p>Through training, they will independently discern and effectively utilize these positional encodings.</p>
<hr>
<h2 id="6-summary">
  6. Summary
  <a class="anchor" href="#6-summary">#</a>
</h2>
<ul>
<li>Positional encoding is essential for embedding sequence order into Transformers.</li>
<li>Unique patterns are added to token embeddings, enabling the model to determine token positions within a sequence.</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#contents">Contents</a></li>
    <li><a href="#0-layer-implementation">0. Layer Implementation</a></li>
    <li><a href="#1-arbitrary-input">1. Arbitrary input</a></li>
    <li><a href="#2-visualization-of-encoded-information-and-output">2. Visualization of encoded information and output</a></li>
    <li><a href="#3-pattern-analysis">3. Pattern analysis</a></li>
    <li><a href="#4-position-information-recovery">4. Position information recovery</a></li>
    <li><a href="#5-is-there-a-need-for-post-processing">5. Is There a Need for Post-Processing?</a></li>
    <li><a href="#6-summary">6. Summary</a></li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>

















<link rel="stylesheet" href="https://crimson206.github.io/data_science_blog/css/styles.min.css">